{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfba77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.187.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting pydantic (from google-generativeai)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting tqdm (from google-generativeai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-generativeai) (4.14.1)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pyparsing<4,>=3.0.4 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google-generativeai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic->google-generativeai)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic->google-generativeai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading google_api_python_client-2.187.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: uritemplate, typing-inspection, tqdm, pyparsing, pydantic-core, pyasn1, protobuf, grpcio, cachetools, annotated-types, rsa, pydantic, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23/23\u001b[0m [google-generativeai]ogle-generativeai]language]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 cachetools-6.2.2 google-ai-generativelanguage-0.6.15 google-api-core-2.28.1 google-api-python-client-2.187.0 google-auth-2.43.0 google-auth-httplib2-0.2.1 google-generativeai-0.8.5 googleapis-common-protos-1.72.0 grpcio-1.76.0 grpcio-status-1.71.2 httplib2-0.31.0 proto-plus-1.26.1 protobuf-5.29.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.12.4 pydantic-core-2.41.5 pyparsing-3.2.5 rsa-4.9.1 tqdm-4.67.1 typing-inspection-0.4.2 uritemplate-4.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0d6746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46693c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "#modelo utilizado\n",
    "MODEL = \"gemini-2.5-flash\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64198489",
   "metadata": {},
   "source": [
    "## PROMPT ZERO SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbbbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "Feito teste zero-shot com texto ID: 19\n",
      "Texto: Os acontecimentos no Nepal e agora no M√©xico mostram de forma clara que ou o Brasil regula as redes sociais ou seremos alvo de um golpe digital, tal qual est√° ocorrendo pelo mundo todo.\n",
      "Fal√°cias: Ladeira Escorregadia, Generaliza√ß√£o Apressada, Falso Dilema, Apelo √† Emo√ß√£o\n",
      "Resposta: O texto apresenta **Falso Dilema**, **Ladeira Escorregadia**, **Generaliza√ß√£o Apressada** e **Apelo √† Emo√ß√£o**. O **Falso Dilema** e a **Ladeira Escorregadia** aparecem em \"ou o Brasil regula as redes sociais ou seremos alvo de um golpe digital\". √â um falso dilema por apresentar apenas duas op√ß√µes extremas como as √∫nicas existentes. √â ladeira escorregadia por sugerir que a n√£o regula√ß√£o *inevitavelmente* levar√° a um \"golpe digital\" sem evid√™ncia dos passos intermedi√°rios. A **Generaliza√ß√£o Apressada** est√° em \"Os acontecimentos no Nepal e agora no M√©xico mostram... tal qual est√° ocorrendo pelo mundo todo\", inferindo uma tend√™ncia global de apenas dois exemplos. O termo \"golpe digital\" √© um **Apelo √† Emo√ß√£o**, usando o medo para pressionar.\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prompt_zero_shot(texto):\n",
    "    return f\"\"\"\n",
    "Tarefa: Identificar fal√°cias l√≥gicas em textos argumentativos.\n",
    "\n",
    "Instru√ß√µes:\n",
    "Dado o texto abaixo, identifique quais as seguintes fal√°cias est√£o presentes nele:\n",
    "- Ad Hominem\n",
    "- Ladeira Escorregadia\n",
    "- Generaliza√ß√£o Apressada\n",
    "- Falso Dilema\n",
    "- Apelo √† Emo√ß√£o\n",
    "- Espantalho\n",
    "\n",
    "Indique em uma linha de texto de no m√°ximo 120 palavras a resposta que deve conter:\n",
    "- As fal√°cias (nomes).\n",
    "- Indique em qual/quais trechos do texto argumentativo as fal√°cias aparecem.\n",
    "- D√™ uma breve explica√ß√£o do por que aquele trecho representa essa fal√°cia.\n",
    "\n",
    "Texto:\n",
    "\\\"\\\"\\\"{texto}\\\"\\\"\\\"\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    model = genai.GenerativeModel(MODEL)\n",
    "    ids_escolhidos = [\"19\"]\n",
    "    exemplos = []\n",
    "    falacias_lista = [\n",
    "    \"Ad Hominem\",\n",
    "    \"Ladeira Escorregadia\",\n",
    "    \"Generaliza√ß√£o Apressada\",\n",
    "    \"Falso Dilema\",\n",
    "    \"Apelo √† Emo√ß√£o\",\n",
    "    \"Espantalho\"\n",
    "    ]\n",
    "\n",
    "    with open(\"../dataset_falacias_logicas.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for coluna in reader: \n",
    "            exemplos.append({\n",
    "                \"id\": coluna[\"id\"],\n",
    "                \"texto\": coluna[\"texto\"]\n",
    "            })\n",
    "        exemplos = [c for c in exemplos if c[\"id\"] in ids_escolhidos]\n",
    "        nome_ids = \"_\".join(ids_escolhidos)\n",
    "        nome_arquivo = f\"zero_shot_respostas_ids_{nome_ids}.csv\"\n",
    "        caminho_arquivo = os.path.join(\"resultados\", nome_arquivo)\n",
    "\n",
    "        if not os.path.exists(\"resultados\"):\n",
    "            print(\"Pasta 'resultados/' n√£o encontrada. Criando agora...\")\n",
    "            os.makedirs(\"resultados\")\n",
    "\n",
    "        if os.path.exists(caminho_arquivo): #verifica se o arquivo ja existe, se existir cria um com _novo, usamos isso pra testes com mesmo numero de shots pra ver o quanto variam as respostas\n",
    "            print(f\"Aviso: o arquivo {caminho_arquivo} j√° existe. Criando nova vers√£o com final '_novo'.\")\n",
    "            base, ext = os.path.splitext(caminho_arquivo)\n",
    "            caminho_arquivo = base + \"_novo\" + ext\n",
    "\n",
    "    with open(caminho_arquivo, \"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=[\"id\", \"texto\",\"falacias_encontradas\", \"resposta\"])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for coluna in exemplos:\n",
    "            texto = coluna[\"texto\"]\n",
    "            prompt = prompt_zero_shot(texto)\n",
    "            response = model.generate_content(prompt)\n",
    "            resposta_texto = \" \".join(response.text.split()).strip() #pra deixar formatado no csv\n",
    "            \n",
    "            #pra fazer a coluna de falacias encontradas no csv\n",
    "            falacias_encontradas = [f for f in falacias_lista if f.lower() in resposta_texto.lower()]\n",
    "            falacias_str = \", \".join(falacias_encontradas)\n",
    "            writer.writerow({\n",
    "                \"id\": coluna[\"id\"],\n",
    "                \"texto\": texto,\n",
    "                \"falacias_encontradas\": falacias_str,\n",
    "                \"resposta\": resposta_texto\n",
    "            })\n",
    "            print(\"\\n===============================\")\n",
    "            print(f\"Feito teste zero-shot com texto ID: {coluna['id']}\")\n",
    "            print(f\"Texto: {texto}\")\n",
    "            print(f\"Fal√°cias: {falacias_str if falacias_str else 'nenhuma detectada'}\")\n",
    "            print(f\"Resposta: {resposta_texto}\")\n",
    "            print(\"===============================\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e4a9f",
   "metadata": {},
   "source": [
    "## PROMPT FEW-SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39adf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EXEMPLOS FEW-SHOT FORMATADOS =====\n",
      " Exemplo:\n",
      "Texto: \"Se voc√™ ama de verdade o Brasil e se importa com o futuro dos seus filhos, precisa protestar contra essa PEC que tenta blindar os parlamentares. Ficar calado agora √© trair a na√ß√£o e deixar que destruam o pa√≠s.\"\n",
      "Falacias encontradas: Apelo √† Emo√ß√£o \n",
      "Resposta: O argumento apela ao patriotismo e o amor pelos filhos dos leitores (Apelo a Emo√ß√£o), tentando manipular suas emo√ß√µes para que se sintam culpados por n√£o protestar contra a PEC de blindagem aos parlamentares.\n",
      "\n",
      "Exemplo:\n",
      "Texto: \"Ou voc√™ apoia a reforma agraria, ou est√° contra o progresso e contra os direitos dos trabalhadores.\"\n",
      "Falacias encontradas: Falso Dilema \n",
      "Resposta: O argumento apresenta apenas duas op√ß√µes totalmente extremas (Falso Dilema), ignorando a possibilidade de haver outras posi√ß√µes intermedi√°rias ou alternativas sobre a quest√£o da reforma agr√°ria.\n",
      "\n",
      "Exemplo:\n",
      "Texto: \"Quem defende que o governo deve investir mais em ci√™ncia est√° apenas querendo gastar dinheiro p√∫blico sem retorno real.\"\n",
      "Falacias encontradas: Generaliza√ß√£o Apressada \n",
      "Resposta: O argumento generaliza a posi√ß√£o de todos os defensores do investimento em ci√™ncia (Generaliza√ß√£o Apressada), sugerindo que todos eles querem gastar dinheiro p√∫blico sem retorno real, sem considerar que muitos podem ter raz√µes v√°lidas e fundamentadas para apoiar o investimento em ci√™ncia.\n",
      "\n",
      "Exemplo:\n",
      "Texto: \"Esse deputado vive mentindo pra imprensa e s√≥ quer aparecer. Ele diz que defende ‚Äòmaior transpar√™ncia‚Äô, mas no fundo quer censurar tudo e calar quem pensa diferente. Pessoas assim n√£o merecem ser levadas a s√©rio.\"\n",
      "Falacias encontradas: Ad Hominem e Espantalho \n",
      "Resposta: O argumento combina um ataque pessoal contra o deputado (Ad Hominem) com uma distor√ß√£o de sua posi√ß√£o sobre transpar√™ncia (Espantalho), sugerindo que ele quer censurar tudo, sem apresentar evid√™ncias concretas para apoiar essas alega√ß√µes.\n",
      "\n",
      "Exemplo:\n",
      "Texto: \"Ou voc√™ apoia a 'limpa' que foi feita no morro do Alem√£o ou est√° do lado dos criminosos que destroem fam√≠lias. Pense nas m√£es que perderam seus filhos! Quem se op√µe a essa medida claramente n√£o tem compaix√£o pelo sofrimento das reais v√≠timas.\"\n",
      "Falacias encontradas: Falso Dilema e Apelo √† Emo√ß√£o \n",
      "Resposta: O argumento apresenta apenas duas op√ß√µes extremas (apoiar a opera√ß√£o policial ou estar do lado dos criminosos) e apela √†s emo√ß√µes dos leitores, sugerindo que se n√£o apoiarem a opera√ß√£o, n√£o t√™m compaix√£o pelas v√≠timas j√° feitas por esses criminosos.\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "Feito teste few-shot com texto 19 - Fal√°cias: Ladeira Escorregadia, Generaliza√ß√£o Apressada, Falso Dilema\n"
     ]
    }
   ],
   "source": [
    "def prompt_few_shot(texto,exemplos_fewshot):\n",
    "    exemplos_formatados = \" \"\n",
    "\n",
    "    for ex in exemplos_fewshot:\n",
    "        exemplos_formatados += (\n",
    "            f\"Exemplo:\\n\"\n",
    "            f\"Texto: \\\"{ex['texto']}\\\"\\n\"\n",
    "            f\"Falacias encontradas: {ex['falacia']} \\n\"\n",
    "            f\"Resposta: {ex['resposta']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    # üîç PRINT PARA DEBUGAR OS EXEMPLOS FEW-SHOT\n",
    "    print(\"\\n===== EXEMPLOS FEW-SHOT FORMATADOS =====\")\n",
    "    print(exemplos_formatados)\n",
    "    print(\"========================================\\n\")\n",
    "\n",
    "    return f\"\"\"\n",
    "\n",
    "Tarefa: Identificar fal√°cias l√≥gicas em textos argumentativos.\n",
    "\n",
    "A seguir est√£o alguns exemplos de como identificar fal√°cias l√≥gicas em textos.\n",
    "\n",
    "{exemplos_formatados}\n",
    "\n",
    "Segue as instru√ß√µes para o pr√≥ximo texto.\n",
    "\n",
    "Instru√ß√µes:\n",
    "Dado o texto abaixo, identifique quais as seguintes fal√°cias est√£o presentes nele:\n",
    "- Ad Hominem\n",
    "- Ladeira Escorregadia\n",
    "- Generaliza√ß√£o Apressada\n",
    "- Falso Dilema\n",
    "- Apelo √† Emo√ß√£o\n",
    "- Espantalho\n",
    "\n",
    "Indique em uma linha de texto de no m√°ximo 120 palavras a resposta que deve conter:\n",
    "- As fal√°cias (nomes).\n",
    "- Indique em qual/quais trechos do texto argumentativo as fal√°cias aparecem.\n",
    "- D√™ uma breve explica√ß√£o do por que aquele trecho representa essa fal√°cia.\n",
    "\n",
    "Texto:\n",
    "\\\"\\\"\\\"{texto}\\\"\\\"\\\"\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    model = genai.GenerativeModel(MODEL)\n",
    "\n",
    "    ids_escolhidos = [\"19\"]  #escolha dos exemplos pra teste few-shot\n",
    "    exemplos = []\n",
    "    ids_exemplos_fewshot = [\"12\",\"13\",\"14\",\"3\",\"6\"]  #exemplos que irao ser usados no prompt few-shot\n",
    "    exemplos_fewshot = []\n",
    "\n",
    "    falacias_lista = [\n",
    "        \"Ad Hominem\",\n",
    "        \"Ladeira Escorregadia\",\n",
    "        \"Generaliza√ß√£o Apressada\",\n",
    "        \"Falso Dilema\",\n",
    "        \"Apelo √† Emo√ß√£o\",\n",
    "        \"Espantalho\"\n",
    "    ]\n",
    "\n",
    "    #le csv\n",
    "    with open(\"../dataset_falacias_logicas.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for coluna in reader:\n",
    "            if coluna[\"id\"] in ids_exemplos_fewshot:\n",
    "                exemplos_fewshot.append({\n",
    "                    \"id\": coluna[\"id\"],\n",
    "                    \"texto\": coluna[\"texto\"],\n",
    "                    \"falacia\": coluna[\"falacias_esperadas\"], \n",
    "                    \"resposta\": coluna[\"resposta_esperada\"]\n",
    "                })\n",
    "\n",
    "            exemplos.append({\n",
    "                \"id\": coluna[\"id\"],\n",
    "                \"texto\": coluna[\"texto\"]    \n",
    "            })\n",
    "\n",
    "        exemplos = [c for c in exemplos if c[\"id\"] in ids_escolhidos]\n",
    "\n",
    "        nome_ids = \"_\".join(ids_escolhidos)\n",
    "        nome_arquivo = f\"few_shot_respostas_ids_{nome_ids}.csv\"\n",
    "        caminho_arquivo = os.path.join(\"resultados\", nome_arquivo)\n",
    "\n",
    "        if not os.path.exists(\"resultados\"):\n",
    "            print(\"Pasta 'resultados/' n√£o encontrada. Criando agora...\")\n",
    "            os.makedirs(\"resultados\")\n",
    "\n",
    "        if os.path.exists(caminho_arquivo):\n",
    "            print(f\"Aviso: o arquivo {caminho_arquivo} j√° existe. Criando nova vers√£o com final '_novo'.\")\n",
    "            base, ext = os.path.splitext(caminho_arquivo)\n",
    "            caminho_arquivo = base + \"_novo\" + ext\n",
    "\n",
    "    with open(caminho_arquivo, \"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=[\"id\", \"texto\", \"falacias_encontradas\", \"resposta\"])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for coluna in exemplos:\n",
    "            texto = coluna[\"texto\"]\n",
    "            prompt = prompt_few_shot(texto, exemplos_fewshot)\n",
    "            response = model.generate_content(prompt)\n",
    "\n",
    "            resposta_texto = \" \".join(response.text.split()).strip()\n",
    "\n",
    "            falacias_encontradas = [f for f in falacias_lista if f.lower() in resposta_texto.lower()]\n",
    "            falacias_str = \", \".join(falacias_encontradas)\n",
    "\n",
    "            writer.writerow({\n",
    "                \"id\": coluna[\"id\"],\n",
    "                \"texto\": texto,\n",
    "                \"falacias_encontradas\": falacias_str,\n",
    "                \"resposta\": resposta_texto\n",
    "            })\n",
    "            print(\"\\n===============================\")\n",
    "            print(f\"Feito teste few-shot com texto ID: {coluna['id']}\")\n",
    "            print(f\"Texto: {texto}\")\n",
    "            print(f\"Fal√°cias: {falacias_str if falacias_str else 'nenhuma detectada'}\")\n",
    "            print(f\"Resposta: {resposta_texto}\")\n",
    "            print(\"===============================\\n\")\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849fe8a",
   "metadata": {},
   "source": [
    "## PROMPT ZERO-SHOT-CHAIN-OF-THOUGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16cfb335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "Feito teste zero-shot com texto ID: 19\n",
      "Texto: Os acontecimentos no Nepal e agora no M√©xico mostram de forma clara que ou o Brasil regula as redes sociais ou seremos alvo de um golpe digital, tal qual est√° ocorrendo pelo mundo todo.\n",
      "Fal√°cias: Generaliza√ß√£o Apressada, Falso Dilema, Apelo √† Emo√ß√£o\n",
      "Resposta: As fal√°cias presentes s√£o Generaliza√ß√£o Apressada, Falso Dilema e Apelo √† Emo√ß√£o. A **Generaliza√ß√£o Apressada** surge ao usar apenas os casos do Nepal e M√©xico para uma conclus√£o abrangente sobre o Brasil e o mundo (\"Os acontecimentos no Nepal e agora no M√©xico mostram de forma clara que...\"). O **Falso Dilema** √© evidente em \"ou o Brasil regula as redes sociais ou seremos alvo de um golpe digital\", que limita as op√ß√µes a apenas duas extremas, ignorando outras possibilidades. O **Apelo √† Emo√ß√£o** se manifesta em \"...seremos alvo de um golpe digital...\", explorando o medo de um evento catastr√≥fico para for√ßar a aceita√ß√£o do argumento, em vez de uma justifica√ß√£o l√≥gica.\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prompt_zero_shot_CoT(texto):\n",
    "    return f\"\"\"\n",
    "Tarefa: Identificar fal√°cias l√≥gicas em textos argumentativos.\n",
    "\n",
    "Instru√ß√µes:\n",
    "Dado o texto abaixo, identifique quais as seguintes fal√°cias est√£o presentes nele:\n",
    "- Ad Hominem\n",
    "- Ladeira Escorregadia\n",
    "- Generaliza√ß√£o Apressada\n",
    "- Falso Dilema\n",
    "- Apelo √† Emo√ß√£o\n",
    "- Espantalho\n",
    "\n",
    "Indique em uma linha de texto de no m√°ximo 120 palavras a resposta que deve conter:\n",
    "- As fal√°cias (nomes).\n",
    "- Indique em qual/quais trechos do texto argumentativo as fal√°cias aparecem.\n",
    "- D√™ uma breve explica√ß√£o do por que aquele trecho representa essa fal√°cia.\n",
    "\n",
    "Texto:\n",
    "\\\"\\\"\\\"{texto}\\\"\\\"\\\"\n",
    "\n",
    "Resposta: \n",
    "(Pense passo a passo para identificar, classificar e explicar as fal√°cias presentes no texto acima.)\n",
    "(Mostre APENAS a resposta final, curta e organizada. N√£o revele o racioc√≠nio.)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    model = genai.GenerativeModel(MODEL)\n",
    "    ids_escolhidos = [\"19\"]\n",
    "    exemplos = []\n",
    "    falacias_lista = [\n",
    "    \"Ad Hominem\",\n",
    "    \"Ladeira Escorregadia\",\n",
    "    \"Generaliza√ß√£o Apressada\",\n",
    "    \"Falso Dilema\",\n",
    "    \"Apelo √† Emo√ß√£o\",\n",
    "    \"Espantalho\"\n",
    "    ]\n",
    "\n",
    "    with open(\"../dataset_falacias_logicas.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for coluna in reader: \n",
    "            exemplos.append({\n",
    "                \"id\": coluna[\"id\"],\n",
    "                \"texto\": coluna[\"texto\"]\n",
    "            })\n",
    "        exemplos = [c for c in exemplos if c[\"id\"] in ids_escolhidos]\n",
    "        nome_ids = \"_\".join(ids_escolhidos)\n",
    "        nome_arquivo = f\"zero_shot_cot_respostas_ids_{nome_ids}.csv\"\n",
    "        caminho_arquivo = os.path.join(\"resultados\", nome_arquivo)\n",
    "\n",
    "        if not os.path.exists(\"resultados\"):\n",
    "            print(\"Pasta 'resultados/' n√£o encontrada. Criando agora...\")\n",
    "            os.makedirs(\"resultados\")\n",
    "\n",
    "        if os.path.exists(caminho_arquivo): #verifica se o arquivo ja existe, se existir cria um com _novo, usamos isso pra testes com mesmo numero de shots pra ver o quanto variam as respostas\n",
    "            print(f\"Aviso: o arquivo {caminho_arquivo} j√° existe. Criando nova vers√£o com final '_novo'.\")\n",
    "            base, ext = os.path.splitext(caminho_arquivo)\n",
    "            caminho_arquivo = base + \"_novo\" + ext\n",
    "\n",
    "    with open(caminho_arquivo, \"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=[\"id\", \"texto\",\"falacias_encontradas\", \"resposta\"])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for coluna in exemplos:\n",
    "            texto = coluna[\"texto\"]\n",
    "            prompt = prompt_zero_shot(texto)\n",
    "            response = model.generate_content(prompt)\n",
    "            resposta_texto = \" \".join(response.text.split()).strip() #pra deixar formatado no csv\n",
    "            \n",
    "            #pra fazer a coluna de falacias encontradas no csv\n",
    "            falacias_encontradas = [f for f in falacias_lista if f.lower() in resposta_texto.lower()]\n",
    "            falacias_str = \", \".join(falacias_encontradas)\n",
    "            writer.writerow({\n",
    "                \"id\": coluna[\"id\"],\n",
    "                \"texto\": texto,\n",
    "                \"falacias_encontradas\": falacias_str,\n",
    "                \"resposta\": resposta_texto\n",
    "            })\n",
    "            print(\"\\n===============================\")\n",
    "            print(f\"Feito teste zero-shot com texto ID: {coluna['id']}\")\n",
    "            print(f\"Texto: {texto}\")\n",
    "            print(f\"Fal√°cias: {falacias_str if falacias_str else 'nenhuma detectada'}\")\n",
    "            print(f\"Resposta: {resposta_texto}\")\n",
    "            print(\"===============================\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
